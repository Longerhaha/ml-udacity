1 什么是pickle?pickle有什么优缺点？
下面的链接有详细的资料
http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/00138683221577998e407bb309542d9b6a68d9276bc3dbe000
序列化：我们把变量从内存中变成可存储或传输的过程称之为序列化，它是一个将任意复杂的对象转成对象的文本或二进制表示的过程。
import pickle
f = open('test.txt','wb')#以二进制写入文件
d = dict(name='Bob', age=20, score=88)
pickle.dump(d, f)
f.close()

f = open('dump.txt', 'rb')#以二进制读取文件，以r方式也可以
origin_data = pickle.load(f)
f.close()

在Python中叫pickling，在其他语言中也被称之为serialization，marshalling，flattening等等，都是一个意思。序列化之后，就可以把序列化后的内容写入磁盘，或者通过网络传输到别的机器上。

反序列化：反过来，把变量内容从序列化的对象重新读到内存里称之为反序列化，即unpickling。
Python提供两个模块来实现序列化：cPickle（c语言写的，速度快）和pickle（纯python速度慢）。用的时候，先尝试导入cPickle，如果失败，再导入pickle

缺点：Pickle的问题和所有其他编程语言特有的序列化问题一样，就是它只能用于Python，并且可能不同版本的Python彼此都不兼容，因此，只能用Pickle保存那些不重要的数据，不能成功地反序列化也没关系。

2、什么是KNN(K最邻近分类算法)，有什么优缺点.
该方法的思路是：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或者
几个样本的类别来决定待分样本所属的类别。 KNN方法虽然从原理上也依赖于极限定理，但在类别决策时，只与极少量的相邻样本有关。由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的。
因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合。
缺点：(1)、该算法在分类时有个主要的不足是，当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。 该算法只计算“最近的”邻居样本，某一类的样本
数量很大，那么或者这类样本并不接近目标样本，或者这类样本很靠近目标样本。无论怎样，数量并不能影响运行结果。可以采用权值的方法（和该样本距离小的邻居权值大）来改进。
(2)、该方法的另一个不足之处是计算量较大，因为对每一个待分类的文本都要计算它到全体已知样本的距离，才能求得它的K个最近邻点。目前常用的解决方法是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本。该算法比较适用于样
本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。
3、什么是随机森林，有什么优缺点？
随机森林由Leo Breiman（2001）提出的一种分类算法，它通过自助法（bootstrap）重采样技术，从原始训练样本集N中有放回地重复随机抽取n个样本生成新的训练样本集合训练决策树，然后按以上步骤生成m棵决策树组成随机森林，新数据的分类结果按分类树投票多少形成的分数而定。其实质是对决策树算法的一种改进，将多个决策树合并在一起，每棵树的建立依赖于独立抽取的样本。 
单棵树的分类能力可能很小，但在随机产生大量的决策树后，一个测试样本可以通过每一棵树的分类结果经统计后选择最可能的分类。 
随机森林大致过程如下： 
1）从样本集中有放回随机采样选出n个样本； 
2）从所有特征中随机选择k个特征，对选出的样本利用这些特征建立决策树（一般是CART，也可是别的或混合）； 
3）重复以上两步m次，即生成m棵决策树，形成随机森林； 
4）对于新数据，经过每棵树决策，最后投票确认分到哪一类。 
随机森林有很多优点： 
1） 每棵树都选择部分样本及部分特征，一定程度避免过拟合； 
2） 每棵树随机选择样本并随机选择特征，使得具有很好的抗噪能力，性能稳定； 
3） 能处理很高维度的数据，并且不用做特征选择； 
4） 适合并行计算； 
5） 实现比较简单； 
缺点： 
1） 参数较复杂； 
2） 模型训练和预测都比较慢。 